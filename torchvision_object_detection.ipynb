{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05ve22CaWR6a",
        "outputId": "f54d5dfc-8449-479e-94ed-246274a88785"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mJov0OuW-Ky"
      },
      "outputs": [],
      "source": [
        "!unzip -q /content/drive/MyDrive/datasets/GP-total-finetune.zip -d ./\n",
        "#!unzip -q /content/drive/MyDrive/datasets/GP-180-roboflow-eval.zip -d ./\n",
        "!unzip -q /content/drive/MyDrive/datasets/GP-180-saurabh-eval.zip -d ./\n",
        "!ln -s /content/drive/MyDrive/datasets/GP-180"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBIoe_tHTQgV",
        "outputId": "33a48a0d-2963-4931-d5ee-279a95a052b5"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "\n",
        "pip install cython\n",
        "# Install pycocotools, the version by default in Colab\n",
        "# has a bug fixed in https://github.com/cocodataset/cocoapi/pull/354\n",
        "pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YG0YyvvuYWBe",
        "outputId": "1a6e1f85-f723-4fa1-e3d4-e89e41f8c2e4"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "\n",
        "# Download TorchVision repo to use some files from\n",
        "# references/detection\n",
        "git clone https://github.com/pytorch/vision.git\n",
        "cd vision\n",
        "git checkout v0.3.0\n",
        "\n",
        "cp references/detection/utils.py ../\n",
        "cp references/detection/transforms.py ../\n",
        "cp references/detection/coco_eval.py ../\n",
        "cp references/detection/engine.py ../\n",
        "cp references/detection/coco_utils.py ../"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3ov2zc0YFPv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.ops as ops\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from torchvision.models.detection import faster_rcnn, rpn, FasterRCNN, backbone_utils,mask_rcnn\n",
        "from torchvision import models,transforms\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "from engine import train_one_epoch, evaluate\n",
        "import utils\n",
        "import transforms as T\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "from copy import deepcopy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nHnEGrvdzlF"
      },
      "outputs": [],
      "source": [
        "epsilon = 1e-7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkIQL9evue49"
      },
      "outputs": [],
      "source": [
        "CPU = torch.device('cpu')\n",
        "GPU = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McjqmPjqb7Zq"
      },
      "outputs": [],
      "source": [
        "hard_recogs = {\n",
        "    \"3009\" : [\"3011\",\"3006\"],\n",
        "    \"3030\" : [\"3002\",\"3029\",\"#3008\"],\n",
        "    \"810\" : [\"716\"],\n",
        "    \"715\" : [\"805\"],\n",
        "    \"881\" : [\"882\"],\n",
        "    \"886\" : [\"887\"],\n",
        "    \"900\" : [\"898\"],\n",
        "    \"908\" : [\"910\"],\n",
        "    \"875\" : [\"874\"],\n",
        "    \"882\" : [\"881\"],\n",
        "    \"960\" : [\"938\"],\n",
        "    \"805\" : [\"715\"],\n",
        "    \"3220\" : [\"3217\"],\n",
        "    \"3251\" : [\"3248\"],\n",
        "    \"3255\" : [\"3205\"],\n",
        "    \"3254\" : [\"3240\",\"3232\"],\n",
        "    \"3240\" : [\"3254\"],\n",
        "    \"3247\" : [\"3234\"],\n",
        "    \"3027\" : [\"3007\"],\n",
        "    \"3007\" : [\"3027\"],\n",
        "    \"3174\" : [\"3133\",\"3144\"]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTgWtixZTs3X"
      },
      "outputs": [],
      "source": [
        "class GroceryProducts(Dataset):\n",
        "    def __init__(self, root, transforms = None):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"images\"))))\n",
        "        self.labels = list(sorted(os.listdir(os.path.join(root, \"labels\"))))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.root, \"images\", self.imgs[idx])\n",
        "        label_path = os.path.join(self.root, \"labels\", self.labels[idx])\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
        "        img = cv2.resize(img,(410,410))\n",
        "        img = img / 255.0\n",
        "        H ,W = img.shape[0], img.shape[1]\n",
        "        img = torch.from_numpy(img).float()\n",
        "        img = img.permute(2,0,1)\n",
        "        boxes = []\n",
        "        annots = []\n",
        "        txt_reader = open(label_path,'r')\n",
        "        for line in txt_reader:\n",
        "            entry = line.split()\n",
        "            annots.append(entry[0])\n",
        "            coords = map(float,entry[1:])\n",
        "            x, y, w, h = coords\n",
        "            x_min = (x - w/2) * W\n",
        "            y_min = (y - h/2) * H\n",
        "            x_max = (x + w/2) * W\n",
        "            y_max = (y + h/2) * H\n",
        "            boxes.append([x_min, y_min, x_max, y_max])\n",
        "\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)   \n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])     \n",
        "        image_id = torch.tensor([idx])\n",
        "        \n",
        "        # As we consider only one class for detection\n",
        "        labels = torch.ones((boxes.shape[0],),dtype = torch.int64)\n",
        "        # None of the instances is crowd (interpret as 'background')\n",
        "        iscrowd = torch.zeros((boxes.shape[0],),dtype = torch.int64)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "        target[\"annots\"] = annots\n",
        "        \n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGkGTfeTiEIW"
      },
      "outputs": [],
      "source": [
        "class RetailDataset(Dataset):\n",
        "    def __init__(self,root_dir,transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.img_list = sorted(os.listdir(root_dir))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.root_dir,self.img_list[idx])\n",
        "        image = cv2.imread(img_path)\n",
        "        #image = load_tf_image(img_path)\n",
        "        if image is None:\n",
        "            print(img_path,idx,self.img_list[idx])\n",
        "        \n",
        "        image = cv2.resize(image,(224,224))\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = torch.from_numpy(image)\n",
        "        image = image.float() / 255\n",
        "        image = torch.permute(image,[2,0,1])\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7AJykjiflkF"
      },
      "outputs": [],
      "source": [
        "#files = sorted(os.listdir(\"/content/GP-180/train\"))\n",
        "REF_DIR = \"/content/GP-180/train\"\n",
        "\n",
        "resnet18 = models.resnet18(pretrained = False)\n",
        "res18 = deepcopy(resnet18)\n",
        "\n",
        "path_to_embed_weights = \"/content/drive/MyDrive/ML_MODELS/resnet18_embed_ep43_full_tune.pt\"\n",
        "\n",
        "tsfm = transforms.Compose([transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])\n",
        "stocks_ds = RetailDataset(REF_DIR,tsfm)\n",
        "stocks_dl = DataLoader(stocks_ds, batch_size = 64, shuffle = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3j-b-mDj5ck",
        "outputId": "ccacdf0f-d871-487b-9884-0dbfbdba6653"
      },
      "outputs": [],
      "source": [
        "ref_itr = iter(stocks_ds)\n",
        "img = next(ref_itr)\n",
        "print(img.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAN40Z6yYvL_",
        "outputId": "ddcc901d-86a4-45fc-94bd-972b69d6c3b9"
      },
      "outputs": [],
      "source": [
        "class ResNet18(nn.Module):\n",
        "    def __init__(self,FREEZE = False):\n",
        "        super(ResNet18,self).__init__()\n",
        "        self.entrypoint = nn.Sequential(res18.conv1,\n",
        "                                        res18.bn1,\n",
        "                                        res18.relu,\n",
        "                                        res18.maxpool)\n",
        "        #self.downConv = nn.MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
        "        self.layer1 = res18.layer1\n",
        "        self.layer2 = res18.layer2\n",
        "        self.layer3 = res18.layer3\n",
        "        self.layer4 = res18.layer4\n",
        "        self.maxpool_b3 = nn.MaxPool2d(kernel_size = (14,14))\n",
        "        self.maxpool_b4 = nn.MaxPool2d(kernel_size = (7,7))\n",
        "        #self.maxpool_b2 = nn.MaxPool2d(kernel_size = (28,28))\n",
        "        #self.fc1 = nn.Linear(in_features = 768,out_features = 1024, bias = True)\n",
        "\n",
        "        if FREEZE:\n",
        "            self.freeze_backbone()\n",
        "\n",
        "    def freeze_backbone(self):\n",
        "        self.entrypoint.requires_grad_(False)\n",
        "        self.layer1.requires_grad_(False)\n",
        "        self.layer2.requires_grad_(False)\n",
        "        self.layer3.requires_grad_(False)\n",
        "        #self.layer4.requires_grad_(False)\n",
        "\n",
        "    def forward(self,X):\n",
        "        X0 = self.entrypoint(X)\n",
        "        X1 = self.layer1(X0)\n",
        "        X2 = self.layer2(X1)\n",
        "        X3 = self.layer3(X2)\n",
        "        X4 = self.layer4(X3)\n",
        "        x3_flat = self.maxpool_b3(X3)\n",
        "        x4_flat = self.maxpool_b4(X4)\n",
        "        out = torch.cat([x3_flat,x4_flat],dim = 1)\n",
        "        out = out.view(X.shape[0],-1)\n",
        "        #out = self.fc1(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "encoder = ResNet18()\n",
        "\n",
        "chkpt = torch.load(path_to_embed_weights)\n",
        "encoder.load_state_dict(chkpt[\"model\"])\n",
        "encoder.to(GPU)\n",
        "print(\"Temporarily moved the encoder to GPU.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56bf1A_KhNIJ",
        "outputId": "4a7bd14d-5479-492f-8126-841e850353e2"
      },
      "outputs": [],
      "source": [
        "def extract_embeddings(dataloader, model, D):\n",
        "\n",
        "    embedder_dim = D\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        embeddings = torch.zeros((len(dataloader.dataset), embedder_dim)).to(GPU)\n",
        "    \n",
        "        k = 0\n",
        "        for (nb,batch) in enumerate(tqdm(dataloader)):\n",
        "            batch = batch.to(GPU)\n",
        "            batch_size = batch.shape[0]\n",
        "            output = model(batch).view(batch_size,-1)\n",
        "            output = output / torch.linalg.norm(output,ord=2,dim =1,keepdim = True)\n",
        "            embeddings[k : k + batch_size] = output\n",
        "            k += batch_size\n",
        "\n",
        "    return embeddings\n",
        "\n",
        "\n",
        "embed_size = 768\n",
        "ref_embeddings = extract_embeddings(stocks_dl,encoder,embed_size)\n",
        "\n",
        "print(\"\\nShape of ref_embeddings is\",ref_embeddings.shape)\n",
        "print(\"Device of ref_embeddings\",ref_embeddings.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZ82t8BPoSkD",
        "outputId": "2f96f45d-f8c7-4e92-8c2a-400b1c87c676"
      },
      "outputs": [],
      "source": [
        "ref_img_list = sorted(os.listdir(REF_DIR))\n",
        "print(ref_img_list)\n",
        "print(len(ref_img_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKUZxkQKpagu"
      },
      "outputs": [],
      "source": [
        "image_to_embed = {}\n",
        "for i in range(len(ref_img_list)):\n",
        "    image_to_embed[ref_img_list[i][:-4]] = ref_embeddings[i]\n",
        "\n",
        "#print(image_to_embed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U81CFX06rOah"
      },
      "outputs": [],
      "source": [
        "def re_rank_by_BRISK(target_img,top_k):\n",
        "    #BRISK\n",
        "    k = len(top_k)\n",
        "    ref_img_arr = []\n",
        "    for imgFile in top_k:\n",
        "        img_path = os.path.join(REF_DIR,imgFile + '.jpg')\n",
        "        img = cv2.imread(img_path,flags = cv2.IMREAD_GRAYSCALE)\n",
        "        img = cv2.resize(img,(410,410))\n",
        "        #display(img,(6,4))\n",
        "        ref_img_arr.append(img)\n",
        "\n",
        "    BRISK = cv2.BRISK_create()\n",
        "\n",
        "    #print(\"\\ntarget_img shape\",target_img.shape)\n",
        "    #print(\"target_img\",target_img.min(),target_img.max(),target_img.mean())\n",
        "    #plt.imshow(target_img)\n",
        "    #plt.show()\n",
        "    \n",
        "    \n",
        "    keypoints_target, descriptors_target = BRISK.detectAndCompute(target_img,None)\n",
        "\n",
        "    bf = cv2.BFMatcher(cv2.NORM_L1, crossCheck=True)\n",
        "    keypoints_ref = []\n",
        "    descriptors_ref = []\n",
        "    matches_arr = []\n",
        "    for ref_img in ref_img_arr:\n",
        "        keypoints_curr, descriptors_curr = BRISK.detectAndCompute(ref_img,None)\n",
        "        matches = bf.match(descriptors_target,descriptors_curr)\n",
        "        matches_arr.append(len(matches))\n",
        "\n",
        "    re_ranked_values, re_ranked_indices = torch.as_tensor(matches_arr).sort(descending = True)\n",
        "    #print(\"Matched keypoints are\",re_ranked_values)\n",
        "    re_ranked_top_k = [top_k[x] for x in re_ranked_indices[:k]] \n",
        "    #feature matching\n",
        "\n",
        "    # print(\"Displaying the top k predictions..\")\n",
        "    # for imgFile in re_ranked_top_k:\n",
        "    #     img_path = os.path.join(REF_DIR,imgFile + '.jpg')\n",
        "    #     img = cv2.imread(img_path,flags = cv2.IMREAD_GRAYSCALE)\n",
        "    #     img = cv2.resize(img,(410,410))\n",
        "    #     display(img,(6,4))\n",
        "        \n",
        "    return re_ranked_top_k\n",
        "\n",
        "class UnNormalize(object):\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
        "        Returns:\n",
        "            Tensor: Normalized image.\n",
        "        \"\"\"\n",
        "        for t, m, s in zip(tensor, self.mean, self.std):\n",
        "            t.mul_(s).add_(m)\n",
        "            # The normalize code -> t.sub_(m).div_(s)\n",
        "        return tensor\n",
        "\n",
        "def preprocess(image,transform = None):\n",
        "    if isinstance(image,torch.Tensor):\n",
        "        image = image.permute(1,2,0)\n",
        "        image = image.cpu().numpy()\n",
        "    image = cv2.resize(image,(224,224))\n",
        "    image = torch.from_numpy(image)\n",
        "    image = image.permute(2,0,1)\n",
        "    if transform:\n",
        "        image = transform(image)\n",
        "    return image\n",
        "\n",
        "def yield_top_k_matches(cropped_img, k = 5, apply_BRISK = False):\n",
        "    cropped_img = preprocess(cropped_img,tsfm).unsqueeze(dim = 0).to(GPU)\n",
        "    cropped_embed = encoder(cropped_img)\n",
        "    \n",
        "    cropped_repeat = torch.cat([cropped_embed for _ in range(len(stocks_ds))],dim = 0)\n",
        "    distances = torch.linalg.norm(cropped_repeat - ref_embeddings, dim = 1)\n",
        "    #assert distances.device == GPU\n",
        "    values,indices = distances.sort()\n",
        "\n",
        "    top_k = [stocks_ds.img_list[x][:-4] for x in indices[:k]] \n",
        "    \n",
        "   \n",
        "    #print(\"shape crop image\",cropped_img.shape)\n",
        "    #print(\"Global descriptor top k\",top_k)\n",
        "    if apply_BRISK:\n",
        "        unorm = UnNormalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "        cropped_img = unorm(cropped_img)\n",
        "    \n",
        "        cropped_img = cropped_img[0].permute(1,2,0).cpu().numpy() * 230\n",
        "        cropped_img = cropped_img.astype(np.uint8)\n",
        "        cropped_img = cv2.cvtColor(cropped_img, cv2.COLOR_RGB2GRAY)\n",
        "        re_ranked_top_k = re_rank_by_BRISK(cropped_img,top_k)\n",
        "        top_k = re_ranked_top_k    \n",
        "    \n",
        "    return top_k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "rhjddOw5xHoC",
        "outputId": "1e83c5af-fb19-43f9-81f5-4d0ca90c4255"
      },
      "outputs": [],
      "source": [
        "img = cv2.imread(\"/content/GP-180/train/268.jpg\")\n",
        "img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
        "img = cv2.resize(img,(410,410))\n",
        "img = img[40:230,30:240,:]\n",
        "img = img / 255.0\n",
        "plt.imshow(img)\n",
        "plt.show()\n",
        "img = torch.from_numpy(img).float()\n",
        "img = img.permute(2,0,1)\n",
        "#img = img.unsqueeze(dim = 0)\n",
        "#print(\"Shape of input image is \",img.shape)\n",
        "pred_cls = yield_top_k_matches(img,k = 5,apply_BRISK = False)\n",
        "print(pred_cls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEARO4B_ye0s"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 8\n",
        "FINE_TUNE_PATH_TRAIN = \"/content/GP-total-finetune/train\"\n",
        "FINE_TUNE_PATH_TEST = \"/content/GP-180-saurabh-eval\"\n",
        "FINE_TUNE_PATH_VAL = \"/content/GP-180-saurabh-eval\"\n",
        "WORKERS_no = 2\n",
        "\n",
        "train_ds = GroceryProducts(FINE_TUNE_PATH_TRAIN)\n",
        "valid_ds = GroceryProducts(FINE_TUNE_PATH_VAL)\n",
        "test_ds = GroceryProducts(FINE_TUNE_PATH_TEST)\n",
        "\n",
        "train_dl = DataLoader(train_ds,\n",
        "                      batch_size = BATCH_SIZE,\n",
        "                      shuffle = True,\n",
        "                      num_workers = WORKERS_no,\n",
        "                      collate_fn = utils.collate_fn)\n",
        "\n",
        "valid_dl = DataLoader(valid_ds,\n",
        "                      batch_size = BATCH_SIZE,\n",
        "                      shuffle = False,\n",
        "                      num_workers = WORKERS_no // 2,\n",
        "                      collate_fn = utils.collate_fn)\n",
        "\n",
        "test_dl =  DataLoader(test_ds,\n",
        "                      batch_size = BATCH_SIZE,\n",
        "                      shuffle = False,\n",
        "                      num_workers = WORKERS_no //2\n",
        "                      ,collate_fn = utils.collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjNHjVMOyYlH"
      },
      "outputs": [],
      "source": [
        "def get_detection_model(num_classes,pre_trained = True):\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=pre_trained)\n",
        "\n",
        "    # get number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    \n",
        "    # replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3hMrV7inhqY"
      },
      "outputs": [],
      "source": [
        "del model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoNSFSipsEJz"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "424d3a653e0844ee89831fb72e47b310",
            "fa4712af5f8a4c7b8441c5473a6e6b98",
            "49235202396d43abac83bf5da3a30bbc",
            "c04a34e0c5f5413fa556dc9c1f630260",
            "a671069b1d644d1eb36bbf3daf7e8537",
            "769cd39f25c040eb835fd73dd5fee8a5",
            "ebdf87064f4540deb3089d76ac865222",
            "e4ab58e98b9947acbededd549c6f890d",
            "c2e886f52a794a97b3748d08e6c97b4c",
            "127aa52f6f7c4caa8ddaf7206054a9b3",
            "3474bf8b4bd843eaa72b7c044d441be3"
          ]
        },
        "id": "R2Y2oYlUdttm",
        "outputId": "8f96f99a-66d4-4a95-d82b-5665b45b7906"
      },
      "outputs": [],
      "source": [
        "# our dataset has two classes only - b\n",
        "num_classes = 2\n",
        "\n",
        "# get the model using our helper function\n",
        "model = get_detection_model(num_classes,pre_trained = True)\n",
        "# move model to the right device\n",
        "#model.to(device)\n",
        "#print(\"model moved to device {0}\".format(device))\n",
        "#print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GI-maFydR7D8"
      },
      "outputs": [],
      "source": [
        "#model.backbone.body.requires_grad_(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXlDf9fgOwus"
      },
      "outputs": [],
      "source": [
        "for parameter in model.backbone.body.parameters():\n",
        "    print(parameter.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtZO9vc3t71i"
      },
      "outputs": [],
      "source": [
        "resnet18 = models.resnet18(pretrained = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bqcjc8hHwjPa"
      },
      "outputs": [],
      "source": [
        "backbone_res18 = nn.Sequential(resnet18.conv1,\n",
        "                               resnet18.bn1,\n",
        "                               resnet18.relu,\n",
        "                               resnet18.maxpool,\n",
        "                               resnet18.layer1,\n",
        "                               resnet18.layer2,\n",
        "                               resnet18.layer3,\n",
        "                               resnet18.layer4,\n",
        "                               resnet18.avgpool)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l96eCpsExR5l",
        "outputId": "0030ab68-e42f-4ade-87b7-478aea4f427c"
      },
      "outputs": [],
      "source": [
        "img = torch.rand(1,3,224,224)\n",
        "output = backbone_res18(img)\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "Pmcp6CZKx84U",
        "outputId": "42842f99-3111-4752-e348-60e05656515a"
      },
      "outputs": [],
      "source": [
        "out = model.backbone.body(img)\n",
        "print(out.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kYLuOkFztw1"
      },
      "outputs": [],
      "source": [
        "backbone_res18 = backbone_utils.resnet_fpn_backbone('resnet18',pretrained = True,trainable_layers = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObNeUGIdyVlM"
      },
      "outputs": [],
      "source": [
        "model.backbone = backbone_res18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SC7mO3Rzb8bm"
      },
      "outputs": [],
      "source": [
        "#model.backbone.body.requires_grad_(False)            # Freezing the backbone resnet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "di8fekQFw_T0"
      },
      "outputs": [],
      "source": [
        "model.rpn._pre_nms_top_n['training'] = 1000\n",
        "model.rpn._post_nms_top_n['training'] = 600\n",
        "model.rpn._pre_nms_top_n['testing'] = 500\n",
        "model.rpn._post_nms_top_n['testing'] = 50\n",
        "#model.roi_heads.nms_thresh = 0.30\n",
        "#model.roi_heads.score_thresh = 0.60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmoDggohcwY_"
      },
      "outputs": [],
      "source": [
        "model.to(GPU)\n",
        "pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoenkCj18C4h"
      },
      "outputs": [],
      "source": [
        "# construct an optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.0005,momentum=0.8, weight_decay=0.0005)\n",
        "#optimizer = torch.optim.Adam(params, lr=0.005, weight_decay=0.0005)\n",
        "warmup_factor = 1. / 1000\n",
        "warmup_iters = min(1000, len(train_dl) - 1)\n",
        "\n",
        "lr_scheduler = utils.warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor)\n",
        "# and a learning rate scheduler which decreases the learning rate by\n",
        "# 10x every 3 epochs\n",
        "#lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "#                                               step_size=3,\n",
        "#                                               gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fo7KrHi-wAzb",
        "outputId": "3f587b95-0e45-4fa0-af24-d5bd45d2e010"
      },
      "outputs": [],
      "source": [
        "_,_ = validate_voc_format(model,valid_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAd56lt4kDxc"
      },
      "source": [
        "And now let's train the model for 10 epochs, evaluating at the end of every epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "at-h4OWK0aoc",
        "outputId": "bd40213a-fde4-439d-e205-495c327854af"
      },
      "outputs": [],
      "source": [
        "# let's train it for 10 epochs\n",
        "num_epochs = 40\n",
        "print_freq = 5\n",
        "eval_freq = 2\n",
        "start_epoch = 31\n",
        "end_epoch = 40\n",
        "\n",
        "for epoch in range(start_epoch,end_epoch + 1):\n",
        "    # train for one epoch, printing every 10 iterations\n",
        "    model.train()\n",
        "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
        "    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
        "    header = 'Epoch: [{}]'.format(epoch)\n",
        "\n",
        "    for images, targets in metric_logger.log_every(train_dl, print_freq, header):\n",
        "        \n",
        "        images = list(image.to(GPU) for image in images)\n",
        "        targets = [{k: v.to(GPU) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model(images, targets)\n",
        "\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        # reduce losses over all GPUs for logging purposes\n",
        "        loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
        "        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
        "\n",
        "        loss_value = losses_reduced.item()\n",
        "\n",
        "        if not math.isfinite(loss_value):\n",
        "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
        "            print(loss_dict_reduced)\n",
        "            sys.exit(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if lr_scheduler is not None:\n",
        "            lr_scheduler.step()\n",
        "\n",
        "        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n",
        "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "    # update the learning rate\n",
        "    lr_scheduler.step()\n",
        "    # evaluate on the test dataset\n",
        "    if epoch % eval_freq == 0:\n",
        "        validate_voc_format(model, valid_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "6305226f0a7448e7b6cca4045575dab0",
            "b70d440409dd4bfab0cf7254eb7b3569",
            "a57230c2ba424b6ca7f922db45de5a9c",
            "8c2799770ca3492eabb99abfd37a4ba6",
            "0366d8596dad4fd8a22aed33100a97ac",
            "083b92d53d41428985be099e8a86d120",
            "b969d98b09864a7a869ac140c0d93eda",
            "8fe9adde917549d693aae196b88adeaa",
            "cff323ea48604602b2ff32e1d385105d",
            "ff9dbb0d44d6428691fe47a7ba6416db",
            "3729b770d82b4a77af4be0cfa3e49453"
          ]
        },
        "id": "sLezTTwimfQD",
        "outputId": "685eb908-79e2-42b1-a479-555299cffb93"
      },
      "outputs": [],
      "source": [
        "num_classes = 2\n",
        "model = get_detection_model(num_classes,pre_trained = False)\n",
        "ckpt = torch.load(\"/content/drive/MyDrive/ML_MODELS/faster_rcnn_res50_fpn_ft39(better-than-ft30).pt\")\n",
        "model.load_state_dict(ckpt[\"model\"])\n",
        "model.to(GPU)\n",
        "print(\"detector moved to gpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzsBJkm7ZHKP"
      },
      "outputs": [],
      "source": [
        "#model.eval()\n",
        "model.rpn._pre_nms_top_n['testing'] = 500\n",
        "model.rpn._post_nms_top_n['testing'] = 30\n",
        "model.roi_heads.nms_thresh = 0.30\n",
        "model.roi_heads.score_thresh = 0.60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RT9ZkE9k38Nk",
        "outputId": "f733e74d-e496-4db0-8042-a24a89a61ac4"
      },
      "outputs": [],
      "source": [
        "predictions,statistics,wrong_recogs = validate_voc_format(model,valid_ds,top_k=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nL8R9inBEZoU",
        "outputId": "d1d02c3d-de02-4d1c-cf0a-46b4508086f7"
      },
      "outputs": [],
      "source": [
        "predictions = validate_ISI_format(model,valid_ds,top_k = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfIiHYCN-zDo",
        "outputId": "812f2026-7439-4c01-fa68-fec5a5f86437"
      },
      "outputs": [],
      "source": [
        "print(len(wrong_recogs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJMfQfbR7faP"
      },
      "outputs": [],
      "source": [
        "def plot_predictions(wrong_preds):\n",
        "    for i in range(len(wrong_preds)):\n",
        "        gtruth = wrong_preds[i][1].cpu().permute(1,2,0).numpy()\n",
        "                \n",
        "        #gtruth = cv2.imread(os.path.join(_DIR,wrong_preds[i][0]))\n",
        "        \n",
        "        img1_path = os.path.join(REF_DIR,wrong_preds[i][2] + \".jpg\")\n",
        "        # print(img_path)\n",
        "        img1 = cv2.imread(img1_path)\n",
        "        #img2 = cv2.imread(os.path.join(ROOT_DIR,preds[i][1] + \".jpg\"))\n",
        "        #img3 = cv2.imread(os.path.join(ROOT_DIR,preds[i][2] + \".jpg\"))\n",
        "\n",
        "        #gtruth = cv2.cvtColor(gtruth,cv2.COLOR_BGR2RGB)\n",
        "        img1 = cv2.cvtColor(img1,cv2.COLOR_BGR2RGB)\n",
        "        #img2 = cv2.cvtColor(img2,cv2.COLOR_BGR2RGB)\n",
        "        #img3 = cv2.cvtColor(img3,cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        fig, axs = plt.subplots(1,2,figsize = (14,6))\n",
        "        axs[0].imshow(gtruth)\n",
        "        axs[0].set_title(wrong_recogs[i][0])\n",
        "\n",
        "        axs[1].imshow(img1)\n",
        "        axs[1].set_title(wrong_recogs[i][2])\n",
        "        \n",
        "        # axs[2].imshow(img2)\n",
        "        # axs[2].set_title(\"Top-2\")\n",
        "        # axs[3].imshow(img3)\n",
        "        # axs[3].set_title(\"Top-3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "H7oFHkNl_DWM",
        "outputId": "bbb09529-9c81-43f8-8118-d8980df8e319"
      },
      "outputs": [],
      "source": [
        "plot_predictions(wrong_recogs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5FzGHmlbhW-",
        "outputId": "bb93cc7e-b236-4724-d2c2-6820e5f36c63"
      },
      "outputs": [],
      "source": [
        "verdicts = [stat[3] for stat in statistics]\n",
        "wrong_detections = 0\n",
        "\n",
        "for verdict in verdicts:\n",
        "    for x in verdict:\n",
        "        if x == -1:\n",
        "            wrong_detections += 1\n",
        "\n",
        "print(\"No. of Wrong detections!\",wrong_detections)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3kB70l7TjQ3"
      },
      "outputs": [],
      "source": [
        "print(\"Mean IoU   Std. IoU\")\n",
        "for stat in statistics:\n",
        "    print(\"{0:.2f}        {1:.2f}\".format(stat[1].mean(),stat[1].std()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oBDnbgiTcRA"
      },
      "outputs": [],
      "source": [
        "plot_pred_gt_side_by_side(valid_ds,predictions,judgements = verdicts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wuRllHecFY3"
      },
      "outputs": [],
      "source": [
        "PATH = \"/content/drive/MyDrive/ML_MODELS\"\n",
        "torch.save({\"model\": model.state_dict(),\n",
        "            \"optimizer\": optimizer.state_dict(),\n",
        "            \"lr_scheduler\":lr_scheduler.state_dict(),\n",
        "            \"epochs\": num_epochs },os.path.join(PATH,\"faster_rcnn_res50_fpn_ft39(better-than-ft30).pt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHwIdxH76uPj"
      },
      "outputs": [],
      "source": [
        "# pick one image from the test set\n",
        "img, target = test_ds[0]\n",
        "\n",
        "model.rpn._post_nms_top_n[\"testing\"] = 50\n",
        "model.roi_heads.nms_thresh = 0.40\n",
        "\n",
        "# put the model in evaluation mode\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    prediction = model([img.to(device)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 594
        },
        "id": "VucHn_obXSXY",
        "outputId": "44a086a1-c61d-404b-86b5-469bc4ac19c8"
      },
      "outputs": [],
      "source": [
        "#display(img.permute(1,2,0).numpy())\n",
        "bboxes = prediction[0][\"boxes\"]\n",
        "gtboxes = target[\"boxes\"]\n",
        "image = img.permute(1,2,0).numpy()\n",
        "thickness = 2\n",
        "\n",
        "color1 = (255,0,0)\n",
        "color2 = (0,0,255)\n",
        "image1 = image.copy()\n",
        "image2 = image.copy()\n",
        "\n",
        "#plt.figure(figsize = (15,20))\n",
        "fig, axs = plt.subplots(1,2,figsize = (20,30),sharex = True)\n",
        "for i in range(bboxes.shape[0]):\n",
        "    start_pt = (int(bboxes[i][0]),int(bboxes[i][1]))\n",
        "    end_pt = (int(bboxes[i][2]),int(bboxes[i][3]))\n",
        "    image1 = cv2.rectangle(image1,start_pt,end_pt,color1,thickness)\n",
        "\n",
        "for i in range(gtboxes.shape[0]):\n",
        "    start_pt = (int(gtboxes[i][0]),int(gtboxes[i][1]))\n",
        "    end_pt = (int(gtboxes[i][2]),int(gtboxes[i][3]))\n",
        "    image2 = cv2.rectangle(image2,start_pt,end_pt,color2,thickness)\n",
        "\n",
        "axs[0].imshow(image1)\n",
        "axs[1].imshow(image2)\n",
        "\n",
        "plt.show()\n",
        "#plot_img_with_boxes(img.permute(1,2,0).numpy(),prediction[0][\"boxes\"])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "torchvision_object_detection.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0366d8596dad4fd8a22aed33100a97ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3729b770d82b4a77af4be0cfa3e49453",
            "placeholder": "​",
            "style": "IPY_MODEL_ff9dbb0d44d6428691fe47a7ba6416db",
            "value": " 97.8M/97.8M [00:01&lt;00:00, 76.9MB/s]"
          }
        },
        "083b92d53d41428985be099e8a86d120": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "127aa52f6f7c4caa8ddaf7206054a9b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3474bf8b4bd843eaa72b7c044d441be3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3729b770d82b4a77af4be0cfa3e49453": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "424d3a653e0844ee89831fb72e47b310": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_49235202396d43abac83bf5da3a30bbc",
              "IPY_MODEL_c04a34e0c5f5413fa556dc9c1f630260",
              "IPY_MODEL_a671069b1d644d1eb36bbf3daf7e8537"
            ],
            "layout": "IPY_MODEL_fa4712af5f8a4c7b8441c5473a6e6b98"
          }
        },
        "49235202396d43abac83bf5da3a30bbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ebdf87064f4540deb3089d76ac865222",
            "placeholder": "​",
            "style": "IPY_MODEL_769cd39f25c040eb835fd73dd5fee8a5",
            "value": "100%"
          }
        },
        "6305226f0a7448e7b6cca4045575dab0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a57230c2ba424b6ca7f922db45de5a9c",
              "IPY_MODEL_8c2799770ca3492eabb99abfd37a4ba6",
              "IPY_MODEL_0366d8596dad4fd8a22aed33100a97ac"
            ],
            "layout": "IPY_MODEL_b70d440409dd4bfab0cf7254eb7b3569"
          }
        },
        "769cd39f25c040eb835fd73dd5fee8a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c2799770ca3492eabb99abfd37a4ba6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cff323ea48604602b2ff32e1d385105d",
            "max": 102530333,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8fe9adde917549d693aae196b88adeaa",
            "value": 102530333
          }
        },
        "8fe9adde917549d693aae196b88adeaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a57230c2ba424b6ca7f922db45de5a9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b969d98b09864a7a869ac140c0d93eda",
            "placeholder": "​",
            "style": "IPY_MODEL_083b92d53d41428985be099e8a86d120",
            "value": "100%"
          }
        },
        "a671069b1d644d1eb36bbf3daf7e8537": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3474bf8b4bd843eaa72b7c044d441be3",
            "placeholder": "​",
            "style": "IPY_MODEL_127aa52f6f7c4caa8ddaf7206054a9b3",
            "value": " 160M/160M [00:02&lt;00:00, 77.8MB/s]"
          }
        },
        "b70d440409dd4bfab0cf7254eb7b3569": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b969d98b09864a7a869ac140c0d93eda": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c04a34e0c5f5413fa556dc9c1f630260": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2e886f52a794a97b3748d08e6c97b4c",
            "max": 167502836,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e4ab58e98b9947acbededd549c6f890d",
            "value": 167502836
          }
        },
        "c2e886f52a794a97b3748d08e6c97b4c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cff323ea48604602b2ff32e1d385105d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4ab58e98b9947acbededd549c6f890d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ebdf87064f4540deb3089d76ac865222": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa4712af5f8a4c7b8441c5473a6e6b98": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff9dbb0d44d6428691fe47a7ba6416db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
