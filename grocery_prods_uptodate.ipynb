{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eB2Wm6v7QZAC",
        "outputId": "60047f93-0900-41e8-98bd-d442414a5231"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrInNn3fmKRp"
      },
      "outputs": [],
      "source": [
        "#!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQSHPH9P0BNx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms,models\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import torch\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "from shutil import copyfile\n",
        "#import wandb\n",
        "from copy import deepcopy\n",
        "from sklearn.manifold import TSNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "HrENGgXAmi8M",
        "outputId": "faf9e5a6-5945-4d32-9eb4-03e7149f1b0d"
      },
      "outputs": [],
      "source": [
        "# wandb.login()\n",
        "# api = wandb.Api()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_ZQBNNq7k6Vr",
        "outputId": "03725198-4113-4459-8b2d-3fced1ce0486"
      },
      "outputs": [],
      "source": [
        "# dataset = api.artifact(\"ankitdipto/CV-in-Retail/GP-Test:v0\")\n",
        "# dataset.download()\n",
        "#MODEL = api.artifact(\"ankitdipto/CV-in-Retail/VGG16_c4_c5-hard-ngtv-GAN:v0\")\n",
        "#MODEL.download()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NusOwU-rP63g"
      },
      "outputs": [],
      "source": [
        "#!unzip -q /content/drive/MyDrive/datasets/TRAIN-embed.zip -d ./\n",
        "#!ls /content/artifacts/GP-Test:v0/GP-180/test | wc -l\n",
        "#!unzip -q \"/content/drive/MyDrive/ML_MODELS/extGAN.zip\" -d ./\n",
        "!ln -s /content/drive/MyDrive/datasets/GP-180\n",
        "!ln -s /content/drive/MyDrive/datasets/Grozi-3.2k-mine/Food\n",
        "!ln -s /content/drive/MyDrive/datasets/Grozi-3.2k-mine/TRAIN-embed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gP-JN5Im9v_J",
        "outputId": "19fb4f65-d1d2-4c9f-b988-5bce809b0778"
      },
      "outputs": [],
      "source": [
        "!ls /content/TRAIN-embed | wc -l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-FwfSdb9z6q"
      },
      "outputs": [],
      "source": [
        "TRAIN_DIR = \"/content/TRAIN-embed\"\n",
        "TEST_DIR = \"/content/GP-180/test\"\n",
        "REF_DIR = \"/content/GP-180/train\"\n",
        "#ROOT_DIR = \"/content/artifacts/GP-Test:v0/GP-180/train\"\n",
        "#TEST_DIR = \"/content/artifacts/GP-Test:v0/GP-180/test\"\n",
        "# SAVE_DIR = \"/content/drive/MyDrive/ML_MODELS\"\n",
        "PATH_LABELS = \"/content/GP-180/GP-180-labels.csv\"\n",
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "GPU = torch.device('cuda',index = 0)\n",
        "CPU = torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzfbsm1aQ1rJ"
      },
      "outputs": [],
      "source": [
        "from GAN.models import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVtxU2bNRuXb",
        "outputId": "3f333bc4-7f1f-4a3c-f501-cac680fd2378"
      },
      "outputs": [],
      "source": [
        "gen = Generator()\n",
        "gan_ckpt_dir = \"/content/GAN/generator\"\n",
        "checkpoint = tf.train.Checkpoint(generator=gen)\n",
        "latest = tf.train.latest_checkpoint(gan_ckpt_dir)\n",
        "checkpoint.restore(latest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8pyL07girP9"
      },
      "outputs": [],
      "source": [
        "def load_tf_image(image_file):\n",
        "  image = tf.io.read_file(image_file)\n",
        "  image = tf.image.decode_jpeg(image)\n",
        "  img = tf.cast(image, tf.float32)\n",
        "  return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oATGJGBXYHh",
        "outputId": "c35c937d-b602-4fcf-9d40-0a2f063803be"
      },
      "outputs": [],
      "source": [
        "img = cv2.imread(\"/content/GP-180/test/s3_112_8.jpg\")\n",
        "print(img.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "id": "oJZ4gzovT2Ab",
        "outputId": "fce1a43a-f7b6-4f9b-e0d6-2fdf594509e5"
      },
      "outputs": [],
      "source": [
        "img_path = \"/content/GP-180/train/1467.jpg\"\n",
        "img = load_tf_image(img_path)\n",
        "\n",
        "img = preprocess_image(img)\n",
        "plt.imshow(img)\n",
        "plt.show()\n",
        "\n",
        "#anchor_img = tf.convert_to_tensor(img, dtype = tf.float32)\n",
        "img = load_tf_image(img_path)\n",
        "#img = cv2.imread(img_path)\n",
        "#print(img.shape)\n",
        "\n",
        "img = randomCrop(img,0.7,0.7)\n",
        "\n",
        "img = tf.convert_to_tensor(img,dtype = tf.float32)\n",
        "img = preprocess_image(img)\n",
        "anchor_img = tf.expand_dims(img, axis = 0)\n",
        "anchor_img = gen(anchor_img,training= True)\n",
        "anchor_img = tf.squeeze(anchor_img,axis=0).numpy() * 0.5 + 0.5\n",
        "# print(anchor_img.shape)\n",
        "print(\"Output Image\",anchor_img.min(),anchor_img.mean(),anchor_img.max())\n",
        "# print(anchor_img)\n",
        "plt.imshow(anchor_img)\n",
        "plt.show()\n",
        "\n",
        "#print(anchor_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMFd-EVsZiJF",
        "outputId": "f5103b2e-a351-4774-9850-9a64192ba49a"
      },
      "outputs": [],
      "source": [
        "img_list = []\n",
        "img_list_test = []\n",
        "img_list_ref = []\n",
        "\n",
        "for (root,dirs,files) in os.walk(TRAIN_DIR):\n",
        "    for file in files:\n",
        "        img_list.append(file)\n",
        "\n",
        "for (root,dirs,files) in os.walk(TEST_DIR):\n",
        "    for file in files:\n",
        "        img_list_test.append(file)\n",
        "\n",
        "for (root,dirs,files) in os.walk(REF_DIR):\n",
        "    for file in files:\n",
        "        img_list_ref.append(file)\n",
        "\n",
        "print(img_list[:20])\n",
        "print(img_list_test[:30])\n",
        "print(img_list_ref[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nT0ypxkmdgRY"
      },
      "outputs": [],
      "source": [
        "!rm -rf ANCHOR_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXXwjjgUIqmw"
      },
      "outputs": [],
      "source": [
        "ANCHOR_DIR = \"/content/drive/MyDrive/datasets/GP-180/anchors\"\n",
        "for img in os.listdir(TRAIN_DIR):\n",
        "    src_path = os.path.join(TRAIN_DIR,img)\n",
        "    anchor_path = os.path.join(ANCHOR_DIR,img[:-4] + \"(a).jpg\")        \n",
        "    \n",
        "    anchor_image = load_tf_image(src_path)\n",
        "    #anchor_image = randomCrop(anchor_image,0.8,0.8)\n",
        "    anchor_image = preprocess_image(anchor_image)\n",
        "    anchor_image = tf.expand_dims(anchor_image, axis = 0)\n",
        "    anchor_image = gen(anchor_image,training= True)\n",
        "    anchor_image = tf.squeeze(anchor_image,axis=0).numpy() * 0.5 + 0.5   # Converting image[-1,1] to image[0,1]\n",
        "    print(anchor_image.shape)\n",
        "    anchor_image = anchor_image * 255.0\n",
        "    anchor_image = cv2.cvtColor(anchor_image,cv2.COLOR_BGR2RGB)\n",
        "    #anchor_image.dtype = np.uint8\n",
        "    print(anchor_path)\n",
        "    cv2.imwrite(anchor_path,anchor_image)\n",
        "    #assert False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8cvYli9fTVJ",
        "outputId": "f6ed9c35-326a-4175-ae3f-001a43bdab11"
      },
      "outputs": [],
      "source": [
        "img2label = {}\n",
        "with open(PATH_LABELS) as csv_head:\n",
        "    data = csv.reader(csv_head,delimiter = ',')\n",
        "    for row in data:\n",
        "        img2label[row[0]] = row[1]\n",
        "print(img2label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivSdJFtoCbIU"
      },
      "outputs": [],
      "source": [
        "# img2label = {}\n",
        "# for img_name in img_list:\n",
        "#     img2label[img_name] = img_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLBBcw8EVi9l"
      },
      "outputs": [],
      "source": [
        "def gauss_blur(img):\n",
        "    sigma = random.randint(0,4)\n",
        "    if sigma > 0:\n",
        "        kernel_size = 6 * sigma + 1\n",
        "    else:\n",
        "        kernel_size = 5\n",
        "    #print(img.shape)\n",
        "    blur = cv2.GaussianBlur(img,(kernel_size,kernel_size),sigma)\n",
        "    return blur\n",
        "\n",
        "def randomCrop(img, crop_ratio_min, crop_ratio_max):\n",
        "    assert crop_ratio_min <= 1.0 and crop_ratio_min > 0\n",
        "    assert crop_ratio_max <= 1.0 and crop_ratio_max > 0\n",
        "    assert crop_ratio_max >= crop_ratio_min\n",
        "    height, width = np.random.uniform(crop_ratio_min,crop_ratio_max,[2])\n",
        "    height = int(height * img.shape[0])\n",
        "    width = int(width * img.shape[1])\n",
        "    x = np.random.randint(0, img.shape[1] - width)\n",
        "    y = np.random.randint(0, img.shape[0] - height)\n",
        "    img = img[y:y+height, x:x+width]\n",
        "    return img\n",
        "\n",
        "def change_brightness_saturation(img, bright_f=30, sat_f=0.3): \n",
        "    hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV).astype(\"float32\")\n",
        "    h, s, v = cv2.split(hsv)\n",
        "    value = np.random.randint(-bright_f,bright_f+1)\n",
        "    v = v + value\n",
        "    v = np.clip(v,0,255)\n",
        "    sat_f = np.random.uniform(1-sat_f, 1+sat_f)\n",
        "    s = s * sat_f\n",
        "    s = np.clip(s,0,255)\n",
        "    final_hsv = cv2.merge((h, s, v))\n",
        "    img = cv2.cvtColor(final_hsv.astype(\"uint8\"), cv2.COLOR_HSV2RGB)\n",
        "    return img\n",
        "\n",
        "def mine_hard_negative(idx):\n",
        "    ref_embed = embedding_positive[idx]\n",
        "    bulk_ref_embed = torch.stack([ref_embed for x in range(embedding_positive.shape[0])])\n",
        "\n",
        "    approx_dist = torch.linalg.norm(bulk_ref_embed - embedding_anchor, dim = 1)\n",
        "    values, indices = approx_dist.sort()\n",
        "    return indices[1]     # the 0th item will be identical ,so the hardest negative is the 1st item \n",
        "\n",
        "def mine_hard_negative_batch(embedding_matrix):\n",
        "    hard_negative_batch = []\n",
        "    #embedding_pos = None\n",
        "    for ref_embed in embedding_matrix:\n",
        "        bulk_ref_embed = torch.stack([ref_embed for x in range(embedding_matrix.shape[0])])\n",
        "        approx_dist = torch.linalg.norm(bulk_ref_embed - embedding_matrix, dim = 1)\n",
        "        values, indices = approx_dist.sort()\n",
        "        hard_negative = embedding_matrix[indices[1]]\n",
        "        hard_negative_batch.append(hard_negative)\n",
        "\n",
        "    hard_negative_batch = torch.stack(hard_negative_batch,dim = 0)\n",
        "    return hard_negative_batch\n",
        "\n",
        "def manipulate(img):\n",
        "    anchor_image = gauss_blur(img) \n",
        "    anchor_image = randomCrop(anchor_image, 0.74, 0.74)\n",
        "    anchor_image = change_brightness_saturation(anchor_image, bright_f=30, sat_f=0.8)\n",
        "    anchor_image = cv2.resize(anchor_image, (224, 224))\n",
        "    \n",
        "    return anchor_image\n",
        "\n",
        "def manipulate_batch(imgs):\n",
        "    anchor_image_batch = []\n",
        "    for img in imgs:\n",
        "        anchor_image = gauss_blur(img) \n",
        "        anchor_image = randomCrop(anchor_image, 0.74, 0.74)\n",
        "        anchor_image = change_brightness_saturation(anchor_image, bright_f=30, sat_f=0.8)\n",
        "        anchor_image = cv2.resize(anchor_image, (224, 224))\n",
        "        anchor_img_batch.append(anchor_image)\n",
        "    \n",
        "    anchor_image_batch = torch.stack(anchor_image,dim = 0)\n",
        "    return anchor_image_batch\n",
        "\n",
        "def QualityCheck(emb_pos,emb_anc, sample_size):\n",
        "    Q = []\n",
        "    indices = random.sample(range(emb_anc.shape[0]), k = sample_size)\n",
        "    for idx in indices:\n",
        "        pos_anc_dist = torch.linalg.norm(emb_pos[idx] - emb_anc[idx], dim =0)\n",
        "        hard_neg_idx = mine_hard_negative(idx)\n",
        "\n",
        "        pos_ngtv_dist = torch.linalg.norm(emb_pos[idx] - emb_anc[hard_neg_idx])\n",
        "        quality = pos_ngtv_dist - pos_anc_dist\n",
        "        Q.append(quality)\n",
        "    \n",
        "    return Q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "gTsJp_Ca7CuV",
        "outputId": "eb0b2d74-aa4e-4579-9dbe-ac5f2ac668a1"
      },
      "outputs": [],
      "source": [
        "\n",
        "class RetailDataset(Dataset):\n",
        "    def __init__(self,root_dir,img_list,transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.img_list = img_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.root_dir,self.img_list[idx])\n",
        "        image = cv2.imread(img_path)\n",
        "        #image = load_tf_image(img_path)\n",
        "        if image is None:\n",
        "            print(img_path,idx,self.img_list[idx])\n",
        "        image = cv2.resize(image,(224,224))\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = torch.from_numpy(image)\n",
        "        image = image.float() / 255\n",
        "        image = torch.permute(image,[2,0,1])\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image\n",
        "\n",
        "class AnchorPositiveFetcher(RetailDataset):\n",
        "    def __init__(self,root_dir,img_list,transform = None):\n",
        "        super().__init__(root_dir,img_list,transform)\n",
        "        \n",
        "    def __getitem__(self, idx_pos):\n",
        "        positive_image = super().__getitem__(idx_pos)                  # The positive image\n",
        "        \n",
        "        anchor_path = os.path.join(self.root_dir,self.img_list[idx_pos])\n",
        "       \n",
        "        #-------------- RANDOM manipulation-----------\n",
        "        \n",
        "        anchor_image = cv2.imread(anchor_path)\n",
        "        anchor_image = cv2.cvtColor(anchor_image,cv2.COLOR_BGR2RGB)\n",
        "        anchor_image = manipulate(anchor_image)\n",
        "        \n",
        "        #---------------------------------------------\n",
        "\n",
        "        anchor_image = torch.from_numpy(anchor_image)\n",
        "        anchor_image = anchor_image.float() / 255.0\n",
        "        anchor_image = torch.permute(anchor_image,[2,0,1])\n",
        "        if self.transform:\n",
        "            anchor_image = self.transform(anchor_image)                # The processed anchor image\n",
        "\n",
        "            \n",
        "        AP_pair = torch.stack([anchor_image,positive_image],dim = 0)\n",
        "        return AP_pair\n",
        "\n",
        "\"\"\"\n",
        "class TripletRETAIL(RetailDataset):\n",
        "    def __init__(self,root_dir,img_list,transform = None):\n",
        "        super().__init__(root_dir,img_list,transform)\n",
        "\n",
        "    \n",
        "        \n",
        "    def __getitem__(self, idx_pos):\n",
        "        positive_image = super().__getitem__(idx_pos)                  # The positive image\n",
        "        \n",
        "        anchor_path = os.path.join(self.root_dir,self.img_list[idx_pos])\n",
        "        \n",
        "        \n",
        "        \n",
        "        # ------------ GAN generation ----------------\n",
        "        # anchor_image = load_tf_image(anchor_path)\n",
        "        # anchor_image = randomCrop(anchor_image,0.8,0.8)\n",
        "        # anchor_image = preprocess_image(anchor_image)\n",
        "        # anchor_image = tf.expand_dims(anchor_image, axis = 0)\n",
        "        # anchor_image = gen(anchor_image,training= True)\n",
        "        # anchor_image = tf.squeeze(anchor_image,axis=0).numpy() * 0.5 + 0.5   # Converting image[-1,1] to image[0,1]\n",
        "        # anchor_image = cv2.resize(anchor_image,(224,224))\n",
        "        #----------------------------------------------\n",
        "\n",
        "        #-------------- RANDOM manipulation-----------\n",
        "        \n",
        "        anchor_image = cv2.imread(anchor_path)\n",
        "        anchor_image = cv2.cvtColor(anchor_image,cv2.COLOR_BGR2RGB)\n",
        "        anchor_image = manipulate(anchor_image)\n",
        "        \n",
        "        #---------------------------------------------\n",
        "\n",
        "        anchor_image = torch.from_numpy(anchor_image)\n",
        "        anchor_image = anchor_image.float() / 255.0\n",
        "        anchor_image = torch.permute(anchor_image,[2,0,1])\n",
        "        if self.transform:\n",
        "            anchor_image = self.transform(anchor_image)                # The processed anchor image\n",
        "\n",
        "        no_classes = super().__len__()\n",
        "        neg_idx = random.randint(0,no_classes - 1)\n",
        "        negative_image = super().__getitem__(neg_idx)                  # The negative image chosen randomly\n",
        "\n",
        "        #idx_hard_neg = mine_hard_negative(idx_pos)\n",
        "        #negative_image = super().__getitem__(idx_hard_neg)\n",
        "\n",
        "        #print(\"anchor image mean\",anchor_image.mean())        \n",
        "        triplet = torch.stack([anchor_image,positive_image,negative_image],dim = 0)\n",
        "        return triplet\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uALdV9HIRtub"
      },
      "outputs": [],
      "source": [
        "#vgg13 = models.vgg13_bn(pretrained = True)\n",
        "#vgg16 = models.vgg16_bn(pretrained = True)\n",
        "#print(vgg16)\n",
        "#resnet18 = models.resnet18(pretrained = True)\n",
        "resnet18 = models.resnet18(pretrained = True)\n",
        "res18 = deepcopy(resnet18)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbKXy6yQCuuu"
      },
      "source": [
        "# Triplet network\n",
        "We'll train a triplet network, that takes an anchor, positive (same class as anchor) and negative (different class than anchor) examples. The objective is to learn embeddings such that the anchor is closer to the positive example than it is to the negative example by some margin value.\n",
        "\n",
        "![alt text](images/anchor_negative_positive.png \"Source: FaceNet\")\n",
        "Source: [2] *Schroff, Florian, Dmitry Kalenichenko, and James Philbin. [Facenet: A unified embedding for face recognition and clustering.](https://arxiv.org/abs/1503.03832) CVPR 2015.*\n",
        "\n",
        "**Triplet loss**:   $L_{triplet}(x_a, x_p, x_n) = max(0, m +  \\lVert f(x_a)-f(x_p)\\rVert_2^2 - \\lVert f(x_a)-f(x_n)\\rVert_2^2$\\)\n",
        "\n",
        "## Steps\n",
        "1. Create a dataset returning triplets - **TripletMNIST** class from *datasets.py*, wrapper for MNIST-like classes\n",
        "2. Define **embedding** *(mapping)* network $f(x)$ - **EmbeddingNet** from *networks.py*\n",
        "3. Define **triplet** network processing triplets - **TripletNet** wrapping *EmbeddingNet*\n",
        "4. Train the network with **TripletLoss** - *losses.py*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIEljFmHN7iE"
      },
      "outputs": [],
      "source": [
        "class Network13(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv_block_3 = vgg13.features[0:21]\n",
        "        self.conv_block_4 = vgg13.features[21:28]\n",
        "        self.conv_block_5 = vgg13.features[28:35]\n",
        "        self.downsample = F.interpolate \n",
        "\n",
        "    def forward(self,x):\n",
        "        x3 = self.conv_block_3(x)\n",
        "        x4 = self.conv_block_4(x3)\n",
        "        x5 = self.conv_block_5(x4)\n",
        "\n",
        "        x4 = self.downsample(x4,size = (7,7))\n",
        "        x3 = self.downsample(x3,size = (7,7))\n",
        "\n",
        "        out = torch.cat([x3,x4,x5],dim = 1)\n",
        "        return out\n",
        "\n",
        "class Network16(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv_block_4 = vgg16.features[0:34]\n",
        "        self.conv_block_5 = vgg16.features[34:44]\n",
        "        #self.upsample = nn.Upsample(scale_factor = 2,mode = \"nearest\") \n",
        "        #self.downsample = F.interpolate\n",
        "        self.maxpool_b4 = nn.MaxPool2d(kernel_size = (14,14))\n",
        "        self.maxpool_b5 = nn.MaxPool2d(kernel_size = (7,7))\n",
        "\n",
        "    def forward(self,x):\n",
        "        x4 = self.conv_block_4(x)\n",
        "        x5 = self.conv_block_5(x4)\n",
        "\n",
        "        #x4 = self.downsample(x4,size = (7,7))\n",
        "        x4 = self.maxpool_b4(x4)\n",
        "        x5 = self.maxpool_b5(x5)\n",
        "        #print(x4.shape,x5.shape)\n",
        "\n",
        "        out = torch.cat([x4,x5],dim = 1)\n",
        "        return out\n",
        "\n",
        "class ResNet18_high_reso(nn.Module):\n",
        "    def __init__(self,FREEZE = False):\n",
        "        super(ResNet18_high_reso,self).__init__()\n",
        "        self.entrypoint = nn.Sequential(res18.conv1,\n",
        "                                        res18.bn1,\n",
        "                                        res18.relu,\n",
        "                                        res18.maxpool)\n",
        "        #self.downConv = nn.MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
        "        self.layer1 = res18.layer1\n",
        "        self.layer2 = res18.layer2\n",
        "        self.layer3 = res18.layer3\n",
        "        self.layer4 = res18.layer4\n",
        "        \n",
        "        self.layer5 = nn.Conv2d(in_channels = 512,out_channels = 512, kernel_size = 3,stride = 1)\n",
        "\n",
        "        self.maxpool_b4 = nn.MaxPool2d(kernel_size = (14,14))\n",
        "        self.maxpool_b5 = nn.MaxPool2d(kernel_size = (7,7))\n",
        "        \n",
        "        #self.fc1 = nn.Linear(in_features = 768,out_features = 1024, bias = True)\n",
        "\n",
        "        if FREEZE:\n",
        "            self.freeze_backbone()\n",
        "\n",
        "    def freeze_backbone(self):\n",
        "        self.entrypoint.requires_grad_(False)\n",
        "        self.layer1.requires_grad_(False)\n",
        "        self.layer2.requires_grad_(False)\n",
        "        self.layer3.requires_grad_(False)\n",
        "        self.layer4.requires_grad_(False)\n",
        "\n",
        "    def forward(self,X):\n",
        "        X0 = self.entrypoint(X)\n",
        "        X1 = self.layer1(X0)\n",
        "        X2 = self.layer2(X1)\n",
        "        X3 = self.layer3(X2)\n",
        "        X4 = self.layer4(X3)\n",
        "        X5 = self.layer5(X4)\n",
        "\n",
        "        #print(X5.shape)\n",
        "        x4_flat = self.maxpool_b4(X4)\n",
        "        x5_flat = self.maxpool_b5(X5)\n",
        "        \n",
        "        out = torch.cat([x4_flat,x5_flat],dim = 1)\n",
        "        #print(out.shape)\n",
        "        #assert False\n",
        "        out = out.view(X.shape[0],-1)\n",
        "        return out\n",
        "\n",
        "class ResNet18(nn.Module):\n",
        "    def __init__(self,FREEZE = False):\n",
        "        super(ResNet18,self).__init__()\n",
        "        self.entrypoint = nn.Sequential(res18.conv1,\n",
        "                                        res18.bn1,\n",
        "                                        res18.relu,\n",
        "                                        res18.maxpool)\n",
        "        #self.downConv = nn.MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
        "        self.layer1 = res18.layer1\n",
        "        self.layer2 = res18.layer2\n",
        "        self.layer3 = res18.layer3\n",
        "        self.layer4 = res18.layer4\n",
        "        self.maxpool_b3 = nn.MaxPool2d(kernel_size = (14,14))\n",
        "        self.maxpool_b4 = nn.MaxPool2d(kernel_size = (7,7))\n",
        "        #self.maxpool_b2 = nn.MaxPool2d(kernel_size = (28,28))\n",
        "        #self.fc1 = nn.Linear(in_features = 768,out_features = 1024, bias = True)\n",
        "\n",
        "        if FREEZE:\n",
        "            self.freeze_backbone()\n",
        "\n",
        "    def freeze_backbone(self):\n",
        "        self.entrypoint.requires_grad_(False)\n",
        "        self.layer1.requires_grad_(False)\n",
        "        self.layer2.requires_grad_(False)\n",
        "        self.layer3.requires_grad_(False)\n",
        "        #self.layer4.requires_grad_(False)\n",
        "\n",
        "    def forward(self,X):\n",
        "        X0 = self.entrypoint(X)\n",
        "        X1 = self.layer1(X0)\n",
        "        X2 = self.layer2(X1)\n",
        "        X3 = self.layer3(X2)\n",
        "        X4 = self.layer4(X3)\n",
        "        \n",
        "        x3_flat = self.maxpool_b3(X3)\n",
        "        x4_flat = self.maxpool_b4(X4)\n",
        "        \n",
        "        out = torch.cat([x3_flat,x4_flat],dim = 1)\n",
        "        out = out.view(X.shape[0],-1)\n",
        "        #out = self.fc1(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBugycEj7cJS"
      },
      "outputs": [],
      "source": [
        "del model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZUsmajFnhfr"
      },
      "outputs": [],
      "source": [
        "#del model\n",
        "model = ResNet18()\n",
        "unfrozen_params = [p for p in model.parameters() if p.requires_grad == True]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gl9mfnALnpMd",
        "outputId": "1414435f-10f2-48b2-de46-7e9615d0c62f"
      },
      "outputs": [],
      "source": [
        "input = torch.rand(1,3,448,448)\n",
        "output = model(input)\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1fu1HXOt8AZ"
      },
      "outputs": [],
      "source": [
        "model.to(GPU)\n",
        "pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNhsw46l9wNH"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 124\n",
        "\n",
        "tsfm = transforms.Compose([transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])\n",
        "# train_ds = TripletRETAIL(TRAIN_DIR,img_list,tsfm)\n",
        "# train_dl = DataLoader(train_ds, batch_size = BATCH_SIZE, shuffle = False)\n",
        "\n",
        "train_ds = AnchorPositiveFetcher(TRAIN_DIR,img_list,tsfm)\n",
        "train_dl = DataLoader(train_ds, batch_size = BATCH_SIZE, shuffle = False)\n",
        "\n",
        "ref_ds = RetailDataset(REF_DIR,img_list_ref,tsfm)\n",
        "ref_dl = DataLoader(ref_ds,batch_size = BATCH_SIZE, shuffle = False)\n",
        "\n",
        "test_ds = RetailDataset(TEST_DIR,img_list_test,tsfm)\n",
        "test_dl = DataLoader(test_ds, batch_size = BATCH_SIZE * 2, shuffle = False)\n",
        "\n",
        "stocks_ds = RetailDataset(TRAIN_DIR,img_list,tsfm)\n",
        "stocks_dl = DataLoader(stocks_ds, batch_size = BATCH_SIZE, shuffle = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jv4DvFucCuuu"
      },
      "outputs": [],
      "source": [
        "MARGIN = 1\n",
        "loss_fn = nn.TripletMarginLoss(margin = MARGIN)\n",
        "INIT_LR = 1e-4\n",
        "EPOCHS = 15\n",
        "trainable_params = [p for p in model.parameters() if p.requires_grad == True]\n",
        "optimizer = optim.Adam(trainable_params, lr = INIT_LR)\n",
        "#scheduler = optim.lr_scheduler.StepLR(optimizer,step_size = 20, gamma = 0.1)\n",
        "#scheduler = lr_scheduler.StepLR(optimizer, 8, gamma=0.1, last_epoch=-1)\n",
        "log_interval = 1\n",
        "test_interval = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "id": "sFkYTK_w-Vai",
        "outputId": "6c76aebb-5dee-4001-ef19-15f20578f144"
      },
      "outputs": [],
      "source": [
        "run = wandb.init(project = \"CV-in-Retail\", name = \"vgg16_bn_c4+c5_pool_hdngtv\")\n",
        "#wandb.watch(model,log_freq = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEkNg5POEVxn"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ok69I7oJEjDH"
      },
      "outputs": [],
      "source": [
        "del embedding_positive\n",
        "del embedding_anchor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zw-IKSGHNUG"
      },
      "outputs": [],
      "source": [
        "itr = iter(stocks_dl)\n",
        "batch = next(itr).to(DEVICE)\n",
        "output = model(batch)\n",
        "embed_size = output.view(BATCH_SIZE,-1).shape[1]\n",
        "#embed_size = 1024\n",
        "embedding_positive = torch.randn(len(train_ds),embed_size).to(GPU)\n",
        "embedding_anchor = torch.randn(len(train_ds),embed_size).to(GPU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7_YXQKESVZ5",
        "outputId": "acca8b53-e0d8-45c9-956a-e8c02c8accb5"
      },
      "outputs": [],
      "source": [
        "#model.to(GPU)\n",
        "print(embed_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuVQR0MLryaI"
      },
      "outputs": [],
      "source": [
        "best_ckpt = {\"model_state\" : model.state_dict(),\n",
        "            \"optimizer_state\" : optimizer.state_dict(),\n",
        "            \"epochs\" : EPOCHS,\n",
        "            \"margin\" : MARGIN,\n",
        "            \"init_lr\" : INIT_LR,\n",
        "            \"loss\" : float(\"inf\")}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dj9AoYpsCuuz"
      },
      "outputs": [],
      "source": [
        "def train(model,start_epoch,stop_epoch):\n",
        "    global best_ckpt\n",
        "    torch.cuda.empty_cache()\n",
        "    model.train()\n",
        "    for epoch in range(start_epoch,stop_epoch):\n",
        "        print(\"epoch \",epoch)\n",
        "        cost = 0\n",
        "        avg_dist = 0\n",
        "        for (i,batch_AP) in enumerate(tqdm(train_dl)):\n",
        "            #print(f\"Epoch {epoch},batch {i}\")\n",
        "            idx = i * BATCH_SIZE\n",
        "            #triple = torch.permute(triple,[1,0,2,3,4])\n",
        "            #triple = triple.to(DEVICE)\n",
        "            \n",
        "            curr_batch_size = batch_AP.shape[0]\n",
        "            batch_AP = batch_AP.to(GPU)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            batch_AP = torch.permute(batch_AP,[1,0,2,3,4])\n",
        "            positive = batch_AP[0]\n",
        "            anchor = batch_AP[1]\n",
        "            \n",
        "            positive_emb = model(positive).view(curr_batch_size,-1)\n",
        "            anchor_emb = model(anchor).view(curr_batch_size,-1)\n",
        "\n",
        "            negative_emb = mine_hard_negative_batch(positive_emb)\n",
        "\n",
        "            #anchor, positive, negative = triple[0],triple[1] ,triple[2]\n",
        "            \n",
        "            \n",
        "            #negative = model(negative).view(curr_batch_size,-1)\n",
        "\n",
        "            positive_emb = positive_emb / torch.linalg.norm(positive_emb,ord=2,dim =1,keepdim = True)\n",
        "            negative_emb = negative_emb / torch.linalg.norm(negative_emb, dim = 1 ,ord=2,keepdim = True)     # The vectors must be normalised to learn richer representations\n",
        "            anchor_emb = anchor_emb / torch.linalg.norm(anchor_emb, dim = 1,ord =2, keepdim = True) \n",
        "\n",
        "            loss = loss_fn(anchor_emb,positive_emb,negative_emb)\n",
        "            loss.backward()\n",
        "            cost += loss.item()\n",
        "        \n",
        "            embedding_anchor[idx:idx + curr_batch_size] = anchor_emb.to(GPU)\n",
        "            embedding_positive[idx :idx + curr_batch_size] = positive_emb.to(GPU)\n",
        "            optimizer.step()\n",
        "        \n",
        "        # scheduler.step()\n",
        "        if best_ckpt[\"loss\"] > cost:\n",
        "            best_ckpt = {\"model_state\" : model.state_dict(),\n",
        "                \"optimizer_state\" : optimizer.state_dict(),\n",
        "                \"epochs\" : epoch,\n",
        "                \"margin\" : MARGIN,\n",
        "                \"init_lr\" : INIT_LR,\n",
        "                \"loss\" : cost}\n",
        "        \n",
        "        if epoch%log_interval == 0:\n",
        "                            \n",
        "            Q = QualityCheck(embedding_positive,embedding_anchor,300)\n",
        "            Q_avg = sum(Q)/len(Q)\n",
        "            \n",
        "            #_,acc1,acc5,_ = test(model,20)\n",
        "            #wandb.log({\"Train Loss\": cost})\n",
        "            #wandb.log({\"Embedding Quality\":Q_avg})\n",
        "            # wandb.log({\"Val Accuracy top 1\":acc1})\n",
        "            # wandb.log({\"Val Accuracy top 5\":acc5})\n",
        "\n",
        "            print(f\"\\nLoss at epoch {epoch} = {cost}, Embedding quality = {Q_avg}\")\n",
        "        \n",
        "        if epoch > 0 and epoch % test_interval == 0:\n",
        "            #pass\n",
        "            print(\"Starting evaluation ....\")\n",
        "            ref_embeddings = extract_embeddings(ref_dl,model,embed_size)\n",
        "            test_embeddings = extract_embeddings(test_dl,model,embed_size)\n",
        "            preds ,acc1, acc5,acc10, acc20 = test(model,ref_embeddings,test_embeddings)\n",
        "            print(\"The accuracies are:\\n Top 1 accuracy = {0}\\n Top 5 accuracy = {1}\\n Top 10 accuracy = {2}\\n Top 20 accuracy = {3}\".format(acc1,acc5,acc10,acc20))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O96casCB5Tkb",
        "outputId": "a2557318-b202-4ca4-e26d-b47559e70649"
      },
      "outputs": [],
      "source": [
        "#test_interval = 1\n",
        "train(model,0,7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOFm9S3UdhS2",
        "outputId": "7f9afa40-14a0-47b6-f2db-87edbee6c5db"
      },
      "outputs": [],
      "source": [
        "Q = QualityCheck(embedding_positive,embedding_anchor,900)\n",
        "Q_avg = sum(Q)/len(Q)\n",
        "print(Q_avg.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIEbqibmgyJw",
        "outputId": "9c08b5b0-163d-4a43-cdd0-c1b782995d45"
      },
      "outputs": [],
      "source": [
        "cost = 0\n",
        "for (i,batch) in enumerate(tqdm(stocks_dl)):\n",
        "    #print(f\"batch {i}\")\n",
        "    idx = i * BATCH_SIZE\n",
        "    #triple = torch.permute(triple,[1,0,2,3,4])\n",
        "    batch = batch.to(DEVICE)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    anchor, positive, negative = batch,batch,batch\n",
        "    curr_batch_size = anchor.shape[0]\n",
        "    # print(curr_batch_size)\n",
        "    positive = model(positive).view(curr_batch_size,-1)\n",
        "    #anchor = positive\n",
        "    #negative = positive\n",
        "    anchor = model(anchor).view(curr_batch_size,-1)\n",
        "    negative = model(negative).view(curr_batch_size,-1)\n",
        "\n",
        "    positive = positive / torch.linalg.norm(positive,ord=2,dim =1,keepdim = True)\n",
        "    negative = negative / torch.linalg.norm(negative, dim = 1 ,ord=2,keepdim = True)     # The vectors must be normalised to learn richer representations\n",
        "    anchor = anchor / torch.linalg.norm(anchor, dim = 1,ord =2, keepdim = True) \n",
        "\n",
        "    loss = loss_fn(anchor,positive,negative)\n",
        "    loss.backward()\n",
        "    #cost += loss.item()\n",
        "\n",
        "    embedding_anchor[idx:idx + curr_batch_size] = anchor.to(GPU)\n",
        "    embedding_pos[idx :idx + curr_batch_size] = positive.to(GPU)\n",
        "    optimizer.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4WLA_cVtPLN"
      },
      "outputs": [],
      "source": [
        "PATH = os.path.join(SAVE_DIR,\"VGG16-triplet:v2.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2aI6Eb3M1za",
        "outputId": "e1f9d096-32b8-4fc3-b315-d9c63845eee8"
      },
      "outputs": [],
      "source": [
        "print(best_ckpt[\"loss\"],best_ckpt[\"epochs\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Uf1_L6ZkdAq"
      },
      "outputs": [],
      "source": [
        "print(best_ckpt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vry-c_ked-jk"
      },
      "outputs": [],
      "source": [
        "last_ckpt = deepcopy(best_ckpt)\n",
        "last_ckpt[\"model_state\"] = model.state_dict()\n",
        "last_ckpt[\"loss\"] = 5.378\n",
        "last_ckpt[\"epochs\"] = 252\n",
        "last_ckpt[\"acc_top_1\"] = 0.241\n",
        "last_ckpt[\"acc_top_5\"] = 0.433\n",
        "last_ckpt[\"acc_top_10\"] = 0.508"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIWlsfouwYF8",
        "outputId": "6cdb65bd-c12c-4ae4-dd27-8dc83b687729"
      },
      "outputs": [],
      "source": [
        "#model2 = Network16()\n",
        "model.load_state_dict(best_ckpt[\"model_state\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGiuMazD_HyW"
      },
      "outputs": [],
      "source": [
        "best_ckpt[\"epochs\"] = 96\n",
        "best_ckpt[\"acc_top_1\"] = 0.37\n",
        "best_ckpt[\"acc_top_5\"] = 0.61\n",
        "best_ckpt[\"acc_top_10\"] = 0.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgpxLkWgfVt6"
      },
      "outputs": [],
      "source": [
        "artifact = wandb.Artifact(\"VGG16_c4_c5-dn-hdngt-proper\",type = \"model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nhNlOUOtxPE",
        "outputId": "974bfdee-b4bb-43a2-98a2-aea465db2d9b"
      },
      "outputs": [],
      "source": [
        "# last_ckpt = {\"Note\" : \"A vgg16 trained on actual dataset without learning rate scheduler\",\n",
        "#             \"model_state\" : model.state_dict(),\n",
        "#             \"optimizer_state\" : optimizer.state_dict(),\n",
        "#             \"epochs\" : EPOCHS,\n",
        "#             \"margin\" : MARGIN,\n",
        "#             \"init_lr\" : INIT_LR,\n",
        "#             \"loss\" : 287.6784973144531}\n",
        "\n",
        "torch.save(last_ckpt,\"/content/VGG16-C4-C5-dn-hdngtv-ep252.pt\")\n",
        "artifact.add_file(\"/content/VGG16-C4-C5-dn-hdngtv-ep252.pt\")\n",
        "run.log_artifact(artifact)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAfL87H1YiHg"
      },
      "outputs": [],
      "source": [
        "!ls /content/GP-embed-74/train | grep -i Food_Tea_54"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AQftdboTt67"
      },
      "outputs": [],
      "source": [
        "def distance(t1,t2):\n",
        "    del_t = t1 - t2\n",
        "    return torch.linalg.vector_norm(del_t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0HMsMem8Ah6"
      },
      "outputs": [],
      "source": [
        "val_ds = RetailDataset(ROOT_DIR,img_list,tsfm)\n",
        "val_dl = DataLoader(val_ds, batch_size = BATCH_SIZE, shuffle = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "4UJmCIOj8hPC",
        "outputId": "b8512fe0-1b1c-4238-ff96-a17107953d8d"
      },
      "outputs": [],
      "source": [
        "sample = torch.permute(val_ds[0],[1,2,0]) * 250\n",
        "sample = torch.clamp(sample,min = 0,max = 255)\n",
        "plt.imshow(sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmAykmaaI3Ar",
        "outputId": "8c089287-fc3a-41d1-c391-7edfd5968b0d"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "model2 = Network16()\n",
        "ckpt = torch.load(\"/content/artifacts/VGG16_c4_c5-hard-ngtv-GAN:v0/VGG16-C4-C5-hard-neg-gan.pt\", map_location = DEVICE)\n",
        "model2.load_state_dict(ckpt[\"model_state\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yfEmnfRNZ0M",
        "outputId": "39edba9a-f339-404d-9495-803aa03a31a4"
      },
      "outputs": [],
      "source": [
        "ckpt = torch.load(\"/content/artifacts/VGG13_c3_c4_c5-hard-ngtv:v0/VGG13-triplet-hard-neg.pt\", map_location = DEVICE)\n",
        "model.load_state_dict(ckpt[\"model_state\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oO79-C09FkXb"
      },
      "outputs": [],
      "source": [
        "#model2 = Network16()\n",
        "model2.to(GPU)\n",
        "pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dz2xh66UCut5"
      },
      "outputs": [],
      "source": [
        "def plot_embeddings(embeddings, targets, xlim=None, ylim=None):\n",
        "    plt.figure(figsize=(10,10))\n",
        "    for i in range(10):\n",
        "        inds = np.where(targets==i)[0]\n",
        "        plt.scatter(embeddings[inds,0], embeddings[inds,1], alpha=0.5, color=colors[i])\n",
        "    if xlim:\n",
        "        plt.xlim(xlim[0], xlim[1])\n",
        "    if ylim:\n",
        "        plt.ylim(ylim[0], ylim[1])\n",
        "    plt.legend(mnist_classes)\n",
        "\n",
        "def extract_embeddings(dataloader, model, D):\n",
        "\n",
        "    embedder_dim = D\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        embeddings = torch.zeros((len(dataloader.dataset), embedder_dim)).to(GPU)\n",
        "    \n",
        "        k = 0\n",
        "        for (nb,batch) in enumerate(tqdm(dataloader)):\n",
        "            batch = batch.to(DEVICE)\n",
        "            batch_size = batch.shape[0]\n",
        "            output = model(batch).view(batch_size,-1)\n",
        "            output = output / torch.linalg.norm(output,ord=2,dim =1,keepdim = True)\n",
        "            embeddings[k : k + batch_size] = output\n",
        "            k += batch_size\n",
        "\n",
        "    return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIRKgyX5lP0Z",
        "outputId": "15b334be-4e91-4734-f353-e525434b1aa3"
      },
      "outputs": [],
      "source": [
        "modelf = ResNet18()\n",
        "modelf.load_state_dict(best_ckpt[\"model_state\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQfr1U51lj4X"
      },
      "outputs": [],
      "source": [
        "_ = modelf.to(GPU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_W83fs7yfsvK",
        "outputId": "f5ce7e0d-d477-4925-96ac-18331b99504a"
      },
      "outputs": [],
      "source": [
        "ref_embeddings = extract_embeddings(ref_dl,modelf,embed_size)\n",
        "test_embeddings = extract_embeddings(test_dl,modelf,embed_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MgLTzW2sceX",
        "outputId": "a414aa32-6b90-4bfd-c693-bfbdb2164458"
      },
      "outputs": [],
      "source": [
        "len(ref_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqvWjWjPHf04",
        "outputId": "237b8a0e-d036-4528-fc51-4f20dc422c4e"
      },
      "outputs": [],
      "source": [
        "#!rm /content/TRAIN-embed/food_2668.jpg\n",
        "\n",
        "img_list = []\n",
        "img_list_test = []\n",
        "\n",
        "for (root,dirs,files) in os.walk(TRAIN_DIR):\n",
        "    for file in files:\n",
        "        img_list.append(file)\n",
        "\n",
        "for (root,dirs,files) in os.walk(TEST_DIR):\n",
        "    for file in files:\n",
        "        img_list_test.append(file)\n",
        "print(img_list[:12])\n",
        "print(img_list_test[:10])\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "tsfm = transforms.Compose([transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])\n",
        "train_ds = TripletRETAIL(TRAIN_DIR,img_list,tsfm)\n",
        "train_dl = DataLoader(train_ds, batch_size = BATCH_SIZE, shuffle = False)\n",
        "\n",
        "test_ds = RetailDataset(TEST_DIR,img_list_test,tsfm)\n",
        "test_dl = DataLoader(test_ds, batch_size = BATCH_SIZE * 2, shuffle = False)\n",
        "\n",
        "stocks_ds = RetailDataset(TRAIN_DIR,img_list,tsfm)\n",
        "stocks_dl = DataLoader(stocks_ds, batch_size = BATCH_SIZE, shuffle = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0ZFSWPvGRYr"
      },
      "outputs": [],
      "source": [
        "#!rm /content/TRAIN-embed/food_2282.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQPg4oYC-nXo"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def validate(model,idx):\n",
        "    outputs = None\n",
        "    model.eval()\n",
        "    for (i,batch) in enumerate(val_dl):\n",
        "        batch = batch.to(DEVICE)\n",
        "        index = i * BATCH_SIZE\n",
        "        curr_batch_size = min(BATCH_SIZE, len(val_ds) - index)\n",
        "        \n",
        "        embed = model(batch).view(curr_batch_size,-1)\n",
        "        embed = embed.cpu()\n",
        "        print(embed.mean())\n",
        "        if outputs is not None:\n",
        "            outputs = torch.cat([outputs,embed],dim = 0)\n",
        "        else:\n",
        "            outputs = embed        \n",
        "    return outputs\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(model,ref_embeddings,test_embeddings,subset_sz = None):\n",
        "    stocks_ds = None\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    correct_top1 = 0\n",
        "    correct_top5 = 0\n",
        "    correct_top10 = 0\n",
        "    correct_top20 = 0\n",
        "\n",
        "    if subset_sz is None:\n",
        "        sz = len(test_ds)\n",
        "    else:\n",
        "        sz = subset_sz\n",
        "\n",
        "    for i in tqdm(range(sz)):\n",
        "        sample_test = test_embeddings[i]\n",
        "        sample_test_bundle = torch.stack([sample_test for _ in range(len(ref_ds))]).to(GPU)\n",
        "        distances = torch.linalg.norm(sample_test_bundle - ref_embeddings, dim = 1)\n",
        "        #print(distances.device)\n",
        "        # assert distances.device == GPU\n",
        "        values,indices = distances.sort()\n",
        "\n",
        "        top20 = [ref_ds.img_list[x][:-4] for x in indices[:20]]\n",
        "        top10 = [ref_ds.img_list[x][:-4] for x in indices[:10]] \n",
        "        top5 = [ref_ds.img_list[x][:-4] for x in indices[:5]]\n",
        "        top1 = ref_ds.img_list[indices[0]][:-4]\n",
        "        \n",
        "        preds.append(top5)\n",
        "        #print(top5)\n",
        "        #print(img2label[test_ds.img_list[i]])\n",
        "        if img2label[test_ds.img_list[i]] in top5:\n",
        "            correct_top5 += 1\n",
        "\n",
        "        if img2label[test_ds.img_list[i]] in top10:\n",
        "            correct_top10 += 1\n",
        "\n",
        "        if img2label[test_ds.img_list[i]] in top20:\n",
        "            correct_top20 += 1\n",
        "\n",
        "        if img2label[test_ds.img_list[i]] == top1:\n",
        "            correct_top1 += 1\n",
        "\n",
        "    return preds,correct_top1/sz,correct_top5/sz,correct_top10/sz,correct_top20/sz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJWW6yGNmJ7_",
        "outputId": "4ee34ed2-edaa-4aa2-e1fc-50ca643094b5"
      },
      "outputs": [],
      "source": [
        "preds ,acc1, acc5,acc10, acc20 = test(modelf,ref_embeddings,test_embeddings)\n",
        "print(\"The accuracies are:\\n Top 1 accuracy = {0}\\n Top 5 accuracy = {1}\\n Top 10 accuracy = {2}\\n Top 20 accuracy = {3}\".format(acc1,acc5,acc10,acc20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmjThxlqToqa"
      },
      "outputs": [],
      "source": [
        "PATH = \"/content/drive/MyDrive/ML_MODELS/resnet18_embed_gp_3.2k_ep6_full_tune.pt\"\n",
        "torch.save({\"model\" : model.state_dict(),\n",
        "            \"epochs\" : 6},PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfjDQBgnkw8D"
      },
      "outputs": [],
      "source": [
        "PATH = \"/content/drive/MyDrive/ML_MODELS/resnet18_embed_gp_3.2k_ep6_OHNM.pt\"\n",
        "torch.save(best_ckpt,PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Svj6ZUA9h7i",
        "outputId": "9d7b6e20-3e77-4b5b-ee0d-197fcde33346"
      },
      "outputs": [],
      "source": [
        "Top 1 accuracy = 0.7729672650475184\n",
        " Top 5 accuracy = 0.8595564941921858\n",
        " Top 10 accuracy = 0.8732840549102429\n",
        " Top 20 accuracy = 0.9028511087645196"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uP9Weg9f5bC8"
      },
      "outputs": [],
      "source": [
        "tSNE = TSNE(n_components=2)\n",
        "ref_embed_2d = tSNE.fit_transform(ref_embeddings.cpu().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        },
        "id": "tOs1i2U_6amN",
        "outputId": "a5560114-cd5d-42e1-c3a1-35421c741ae2"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (15,12))\n",
        "plt.scatter(ref_embed_2d[:,0],ref_embed_2d[:,1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zEAlcfqiqhB"
      },
      "outputs": [],
      "source": [
        "def plot_predictions(preds):\n",
        "    for i in range(20):\n",
        "        gtruth = cv2.imread(os.path.join(TEST_DIR,img_list_test[i]))\n",
        "        img1 = cv2.imread(os.path.join(ROOT_DIR,preds[i][0] + \".jpg\"))\n",
        "        img2 = cv2.imread(os.path.join(ROOT_DIR,preds[i][1] + \".jpg\"))\n",
        "        img3 = cv2.imread(os.path.join(ROOT_DIR,preds[i][2] + \".jpg\"))\n",
        "\n",
        "        gtruth = cv2.cvtColor(gtruth,cv2.COLOR_BGR2RGB)\n",
        "        img1 = cv2.cvtColor(img1,cv2.COLOR_BGR2RGB)\n",
        "        img2 = cv2.cvtColor(img2,cv2.COLOR_BGR2RGB)\n",
        "        img3 = cv2.cvtColor(img3,cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        fig, axs = plt.subplots(1,4,figsize = (14,6))\n",
        "        axs[0].imshow(gtruth)\n",
        "        axs[0].set_title(\"Groud Truth\")\n",
        "\n",
        "        axs[1].imshow(img1)\n",
        "        axs[1].set_title(\"Top-1\")\n",
        "        \n",
        "        axs[2].imshow(img2)\n",
        "        axs[2].set_title(\"Top-2\")\n",
        "        axs[3].imshow(img3)\n",
        "        axs[3].set_title(\"Top-3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "F3HyTDLQac_u",
        "outputId": "c5e7c930-d685-4fb0-bac6-9a6686d2306f"
      },
      "outputs": [],
      "source": [
        "plot_predictions(preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8EdVB4xBxWL",
        "outputId": "3bfab479-594c-4a29-9b7d-0584d26a7a23"
      },
      "outputs": [],
      "source": [
        "outputs = validate(model,0)\n",
        "print(outputs.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tb1wbA8oWLA2",
        "outputId": "c2fc8469-6a8e-486a-97f0-e892bfd7ce24"
      },
      "outputs": [],
      "source": [
        "print(outputs[18].mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSXPGGjOohRM",
        "outputId": "f64f58ef-8677-430a-a0cb-e1cf2dbe8fb7"
      },
      "outputs": [],
      "source": [
        "print(len(stocks_ds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESxjfGVHQuLT",
        "outputId": "5c536971-ff30-4366-c9a4-d908225dc319"
      },
      "outputs": [],
      "source": [
        "#model.to(DEVICE)\n",
        "model.to(torch.device('cpu'))\n",
        "#del model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vA1JDCxfPUc9",
        "outputId": "bf7bf10d-0054-4610-aadb-49d6b6d1f9b2"
      },
      "outputs": [],
      "source": [
        "anchor_image = torch.permute(val_ds[0],[1,2,0]).numpy() * 255\n",
        "anchor_image = gauss_blur(anchor_image) \n",
        "anchor_image = randomCrop(anchor_image, 0.8, 0.8)\n",
        "anchor_image = change_brightness_saturation(anchor_image, bright_f=40, sat_f=0.8)\n",
        "anchor_image = cv2.resize(anchor_image, (224, 224))\n",
        "anchor_image = torch.from_numpy(anchor_image).float()\n",
        "anchor_image = torch.permute(anchor_image,[2,0,1])\n",
        "anchor_image = val_ds.transform(anchor_image) / 255.0\n",
        "with torch.no_grad():\n",
        "    sample = anchor_image\n",
        "    sample = torch.stack([sample for i in range(len(val_ds))]).to(DEVICE)\n",
        "    ref_embed = model(sample).view(len(val_ds),-1)\n",
        "    print(ref_embed.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6qBi2zAjxFK",
        "outputId": "76d41695-92dd-409d-af6b-61137a948a9b"
      },
      "outputs": [],
      "source": [
        "ref_embed = ref_embed.to(torch.device('cpu'))\n",
        "print(ref_embed.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PSpZtbZQ6aw",
        "outputId": "d802862d-133b-4894-8cc5-ba49fbba2341"
      },
      "outputs": [],
      "source": [
        "#distances = distance(outputs,ref_embed)\n",
        "distances = torch.linalg.norm(outputs - ref_embed, dim = 1)\n",
        "values,indices = distances.sort()\n",
        "print(values[:11])\n",
        "print(indices[:11])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAxIa_8gkNYq",
        "outputId": "4a2003d3-e9fa-4a05-f88d-3118e2cfdab5"
      },
      "outputs": [],
      "source": [
        "print(distances[:7])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "exHyMU_FTcIJ",
        "outputId": "684becb7-7081-4cfb-e877-2cf435bf522d"
      },
      "outputs": [],
      "source": [
        "plt.plot(distances)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EucF8iKGehPf",
        "outputId": "d9923096-c44a-4e53-ea47-3446c06f82a1"
      },
      "outputs": [],
      "source": [
        "print(distances)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oyf-lnwzJaLz",
        "outputId": "225665c3-c8ad-4fa0-c439-fd2e2a1ae0a0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0j5hmWuAt4k",
        "outputId": "2442bb31-39d3-44a7-f0b7-7fc4a98b9dbd"
      },
      "outputs": [],
      "source": [
        "t = torch.stack([val_ds[0] for _ in range(10)])\n",
        "print(t.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1167
        },
        "collapsed": true,
        "id": "ysh4Ry7ZCuu_",
        "outputId": "2ea247e2-eb96-405a-aa5a-b3c0b565d2fd"
      },
      "outputs": [],
      "source": [
        "train_embeddings_tl, train_labels_tl = extract_embeddings(train_loader, model)\n",
        "plot_embeddings(train_embeddings_tl, train_labels_tl)\n",
        "val_embeddings_tl, val_labels_tl = extract_embeddings(test_loader, model)\n",
        "plot_embeddings(val_embeddings_tl, val_labels_tl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7C9H1_nCuvJ"
      },
      "source": [
        "# Online pair/triplet selection - negative mining\n",
        "There are couple of problems with siamese and triplet networks.\n",
        "1. The number of possible pairs/triplets grows **quadratically/cubically** with the number of examples. It's infeasible to process them all\n",
        "2. We generate pairs/triplets randomly. As the training continues, more and more pairs/triplets are easy to deal with (their loss value is very small or even 0), preventing the network from training. We need to provide the network with **hard examples**.\n",
        "3. Each image that is fed to the network is used only for computation of contrastive/triplet loss for only one pair/triplet. The computation is somewhat wasted; once the embedding is computed, it could be reused for many pairs/triplets.\n",
        "\n",
        "To deal with that efficiently, we'll feed a network with standard mini-batches as we did for classification. The loss function will be responsible for selection of hard pairs and triplets within mini-batch. In these case, if we feed the network with 16 images per 10 classes, we can process up to $159*160/2 = 12720$ pairs and $10*16*15/2*(9*16) = 172800$ triplets, compared to 80 pairs and 53 triplets in previous implementation.\n",
        "\n",
        "We can find some strategies on how to select triplets in [2] and [3] *Alexander Hermans, Lucas Beyer, Bastian Leibe, [In Defense of the Triplet Loss for Person Re-Identification](https://arxiv.org/pdf/1703.07737), 2017*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k806qej9CuvL"
      },
      "source": [
        "## Online pair selection\n",
        "## Steps\n",
        "1. Create **BalancedBatchSampler** - samples $N$ classes and $M$ samples *datasets.py*\n",
        "2. Create data loaders with the batch sampler\n",
        "3. Define **embedding** *(mapping)* network $f(x)$ - **EmbeddingNet** from *networks.py*\n",
        "4. Define a **PairSelector** that takes embeddings and original labels and returns valid pairs within a minibatch\n",
        "5. Define **OnlineContrastiveLoss** that will use a *PairSelector* and compute *ContrastiveLoss* on such pairs\n",
        "6. Train the network!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "goVi1_-PCuvL"
      },
      "outputs": [],
      "source": [
        "from datasets import BalancedBatchSampler\n",
        "\n",
        "# We'll create mini batches by sampling labels that will be present in the mini batch and number of examples from each class\n",
        "train_batch_sampler = BalancedBatchSampler(train_dataset.train_labels, n_classes=10, n_samples=25)\n",
        "test_batch_sampler = BalancedBatchSampler(test_dataset.test_labels, n_classes=10, n_samples=25)\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
        "online_train_loader = torch.utils.data.DataLoader(train_dataset, batch_sampler=train_batch_sampler, **kwargs)\n",
        "online_test_loader = torch.utils.data.DataLoader(test_dataset, batch_sampler=test_batch_sampler, **kwargs)\n",
        "\n",
        "# Set up the network and training parameters\n",
        "from networks import EmbeddingNet\n",
        "from losses import OnlineContrastiveLoss\n",
        "from utils import AllPositivePairSelector, HardNegativePairSelector # Strategies for selecting pairs within a minibatch\n",
        "\n",
        "margin = 1.\n",
        "embedding_net = EmbeddingNet()\n",
        "model = embedding_net\n",
        "if cuda:\n",
        "    model.cuda()\n",
        "loss_fn = OnlineContrastiveLoss(margin, HardNegativePairSelector())\n",
        "lr = 1e-3\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, 8, gamma=0.1, last_epoch=-1)\n",
        "n_epochs = 20\n",
        "log_interval = 250"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1037
        },
        "collapsed": true,
        "id": "SacMTqN6CuvO",
        "outputId": "87a2c414-a641-4cde-ac89-96e7c10cef02"
      },
      "outputs": [],
      "source": [
        "all_embeddings = fit(online_train_loader, online_test_loader, model, loss_fn, optimizer, scheduler, n_epochs, cuda, log_interval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1167
        },
        "collapsed": true,
        "id": "ZCvF0AlCCuvX",
        "outputId": "27de240c-eb68-4de0-b57b-f386ccb23ebb"
      },
      "outputs": [],
      "source": [
        "train_embeddings_ocl, train_labels_ocl = extract_embeddings(train_loader, model)\n",
        "plot_embeddings(train_embeddings_ocl, train_labels_ocl)\n",
        "val_embeddings_ocl, val_labels_ocl = extract_embeddings(test_loader, model)\n",
        "plot_embeddings(val_embeddings_ocl, val_labels_ocl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgIGiMwICuvn"
      },
      "source": [
        "## Online triplet selection\n",
        "## Steps\n",
        "1. Create **BalancedBatchSampler** - samples $N$ classes and $M$ samples *datasets.py*\n",
        "2. Create data loaders with the batch sampler\n",
        "3. Define **embedding** *(mapping)* network $f(x)$ - **EmbeddingNet** from *networks.py*\n",
        "4. Define a **TripletSelector** that takes embeddings and original labels and returns valid triplets within a minibatch\n",
        "5. Define **OnlineTripletLoss** that will use a *TripletSelector* and compute *TripletLoss* on such pairs\n",
        "6. Train the network!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzpYzMUuCuvp"
      },
      "outputs": [],
      "source": [
        "from datasets import BalancedBatchSampler\n",
        "\n",
        "# We'll create mini batches by sampling labels that will be present in the mini batch and number of examples from each class\n",
        "train_batch_sampler = BalancedBatchSampler(train_dataset.train_labels, n_classes=10, n_samples=25)\n",
        "test_batch_sampler = BalancedBatchSampler(test_dataset.test_labels, n_classes=10, n_samples=25)\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
        "online_train_loader = torch.utils.data.DataLoader(train_dataset, batch_sampler=train_batch_sampler, **kwargs)\n",
        "online_test_loader = torch.utils.data.DataLoader(test_dataset, batch_sampler=test_batch_sampler, **kwargs)\n",
        "\n",
        "# Set up the network and training parameters\n",
        "from networks import EmbeddingNet\n",
        "from losses import OnlineTripletLoss\n",
        "from utils import AllTripletSelector,HardestNegativeTripletSelector, RandomNegativeTripletSelector, SemihardNegativeTripletSelector # Strategies for selecting triplets within a minibatch\n",
        "from metrics import AverageNonzeroTripletsMetric\n",
        "\n",
        "margin = 1.\n",
        "embedding_net = EmbeddingNet()\n",
        "model = embedding_net\n",
        "if cuda:\n",
        "    model.cuda()\n",
        "loss_fn = OnlineTripletLoss(margin, RandomNegativeTripletSelector(margin))\n",
        "lr = 1e-3\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, 8, gamma=0.1, last_epoch=-1)\n",
        "n_epochs = 20\n",
        "log_interval = 150"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1377
        },
        "collapsed": true,
        "id": "W-bDxqVJCuvs",
        "outputId": "1ef0d7ce-6c6a-4dc0-e879-348e081f1bd3"
      },
      "outputs": [],
      "source": [
        "fit(online_train_loader, online_test_loader, model, loss_fn, optimizer, scheduler, n_epochs, cuda, log_interval, metrics=[AverageNonzeroTripletsMetric()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNrbA2hCCuvw"
      },
      "outputs": [],
      "source": [
        "train_embeddings_otl, train_labels_otl = extract_embeddings(train_loader, model)\n",
        "plot_embeddings(train_embeddings_otl, train_labels_otl)\n",
        "val_embeddings_otl, val_labels_otl = extract_embeddings(test_loader, model)\n",
        "plot_embeddings(val_embeddings_otl, val_labels_otl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDeV-K4o7Bk2"
      },
      "outputs": [],
      "source": [
        "display_emb_online, display_emb, display_label_online, display_label = train_embeddings_ocl, train_embeddings_cl, train_labels_ocl, train_labels_cl\n",
        "# display_emb_online, display_emb, display_label_online, display_label = val_embeddings_ocl, val_embeddings_cl, val_labels_ocl, val_labels_cl\n",
        "\n",
        "x_lim = (np.min(display_emb_online[:,0]), np.max(display_emb_online[:,0]))\n",
        "y_lim = (np.min(display_emb_online[:,1]), np.max(display_emb_online[:,1]))\n",
        "x_lim = (min(x_lim[0], np.min(display_emb[:,0])), max(x_lim[1], np.max(display_emb[:,0])))\n",
        "y_lim = (min(y_lim[0], np.min(display_emb[:,1])), max(y_lim[1], np.max(display_emb[:,1])))\n",
        "\n",
        "plot_embeddings(display_emb, display_label, x_lim, y_lim)\n",
        "plot_embeddings(display_emb_online, display_label_online, x_lim, y_lim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tNY7NReM0zsq"
      },
      "outputs": [],
      "source": [
        "display_emb_online, display_emb, display_label_online, display_label = train_embeddings_otl, train_embeddings_tl, train_labels_otl, train_labels_tl\n",
        "# display_emb_online, display_emb, display_label_online, display_label = val_embeddings_otl, val_embeddings_tl, val_labels_otl, val_labels_tl\n",
        "x_lim = (np.min(display_emb_online[:,0]), np.max(display_emb_online[:,0]))\n",
        "y_lim = (np.min(display_emb_online[:,1]), np.max(display_emb_online[:,1]))\n",
        "x_lim = (min(x_lim[0], np.min(display_emb[:,0])), max(x_lim[1], np.max(display_emb[:,0])))\n",
        "y_lim = (min(y_lim[0], np.min(display_emb[:,1])), max(y_lim[1], np.max(display_emb[:,1])))\n",
        "plot_embeddings(display_emb, display_label, x_lim, y_lim)\n",
        "plot_embeddings(display_emb_online, display_label_online, x_lim, y_lim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AOfKw7W7VwT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "k806qej9CuvL",
        "UgIGiMwICuvn"
      ],
      "name": "grocery_prods_uptodate.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
